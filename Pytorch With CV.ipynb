{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport time\nimport seaborn as sns\nimport random\n%matplotlib inline\n\n\nimport cv2\nimport torch\nfrom PIL import Image,ImageFile\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset,DataLoader,Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nimport albumentations\nfrom albumentations import torch as AT\nfrom torch.optim.optimizer import Optimizer\n\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n#set the seed\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n#Import the labels,\ntrain_labels=pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\ntest=pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\nsample=pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")\n\n## setting the model:\n\nNUM_CLASS=5\nBATCH_SIZE=64\nIMAGE_SIZE=224\ndata_dir=\"../input/aptos2019-blindness-detection/\"\n\n#cv split:\nn_splits=5\nsplits=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=15)\n\n## Import the data:\nclass eye(Dataset):\n    def __init__(self,labels,directory,subset=False,transform=None):\n        self.labels=labels\n        self.directory=directory\n        self.transform=transform\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self,idx):\n        img_name='{}.png'.format(self.labels.iloc[idx,0])  \n        full_image_path=join(self.directory,img_name)\n        #print(f'\\nImage path:{full_image_path}')\n        #img = cv2.imread(img_name)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #image = self.transform(img)\n        #image = image['image']\n        image=Image.open(full_image_path)\n        image_resize = image.resize((224, 224), resample=Image.BILINEAR)\n        image=self.transform(image)\n        #image=image['image']\n        \n        image_label=self.labels.iloc[idx,1:].as_matrix().astype('float')\n        image_label=np.argmax(image_label)\n        return [image,image_label]\n    \n    \n\ntrain_trans=transforms.Compose([transforms.RandomCrop(224),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(degrees=10),transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485,0.485,0.485],std=[0.485,0.485,0.485])])\ntest_trans=transforms.Compose([transforms.RandomCrop(224),transforms.ToTensor(),\n                                #transforms.RandomHorizontalFlip(),\n                                #transforms.RandomRotation(),\n                                transforms.Normalize(mean=[0.485,0.485,0.485],std=[0.485,0.485,0.485])])\n# train_ds=eye(train_x,data_dir+'/train_images/',transform=train_trans)\n# valid_ds=eye(valid_x,data_dir+'/train_images/',transform=train_trans)\n# test_ds=eye(sample,data_dir+'/test_images/',transform=train_trans)\n\n# train_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dl=DataLoader(valid_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# test_dl=DataLoader(test_ds,shuffle=True,num_workers=4)\n\n\nmodel=models.resnet50()\nmodel.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet50-19c8e357.pth\"))\n    \n    \n    \nfinal_in_features=model.fc.in_features\nmodel.fc=nn.Linear(final_in_features,10,bias=True)\n#print(model)\n\ndevice=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#taken from https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n# This code is from https://github.com/thomasjpfan/pytorch/blob/401ec389db2c9d2978917a6e4d1101b20340d7e7/torch/optim/lr_scheduler.py\n# This code is under review at PyTorch and is to be merged eventually to make CLR available to all.\n# Tested with pytorch 0.2.0\n\n\n\n\nclass CyclicLR(object):\n    \"\"\"Sets the learning rate of each parameter group according to\n    cyclical learning rate policy (CLR). The policy cycles the learning\n    rate between two boundaries with a constant frequency, as detailed in\n    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n    The distance between the two boundaries can be scaled on a per-iteration\n    or per-cycle basis.\n    Cyclical learning rate policy changes the learning rate after every batch.\n    `batch_step` should be called after a batch has been used for training.\n    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n    This class has three built-in policies, as put forth in the paper:\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n        cycle iteration.\n    This implementation was adapted from the github repo: `bckenstler/CLR`_\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        base_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for eachparam groups.\n            Default: 0.001\n        max_lr (float or list): Upper boundaries in the cycle for\n            each parameter group. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function. Default: 0.006\n        step_size (int): Number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch. Default: 2000\n        mode (str): One of {triangular, triangular2, exp_range}.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n            Default: 'triangular'\n        gamma (float): Constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n            Default: 1.0\n        scale_fn (function): Custom scaling policy defined by a single\n            argument lambda function, where\n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored\n            Default: None\n        scale_mode (str): {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle).\n            Default: 'cycle'\n        last_batch_iteration (int): The index of the last batch. Default: -1\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.CyclicLR(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         scheduler.batch_step()\n        >>>         train_batch(...)\n    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n    \"\"\"\n\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n    \n    \nmax_epochs=5\n\ndef train_model(model,x_train,x_val):\n    optimizer = torch.optim.SGD(model.parameters(), lr=1e-5, momentum=0.99)\n    learn_rate=CyclicLR(optimizer, base_lr=1e-5, max_lr=2e-5,step_size=40, mode='triangular2',gamma=0.99994)\n    \n    train_ds=eye(x_train,data_dir+'/train_images/',transform=train_trans)\n    valid_ds=eye(x_val,data_dir+'/train_images/',transform=test_trans)\n    \n    train=torch.utils.data.DataLoader(train_ds,batch_size=64,shuffle=True)\n    valid=torch.utils.data.DataLoader(valid_ds,batch_size=64,shuffle=True)\n    dataiter=iter(train)\n    img,label=next(dataiter)\n    print(\"\\n Shape of image\",img.shape)\n    #print(\"\\n Shape of label\",label[1].item())\n    criterion=nn.CrossEntropyLoss()\n    \n    for epoch in range(max_epochs):\n        print(time.ctime(),'Epoch:',epoch+1)\n        loss_arr=[]\n        loss_epoch_arr=[]\n        for i, (inputs,labels) in enumerate(tqdm(train,0)):\n        \n        #inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            for param in model.parameters():\n                param.requires_grad=True\n            #https://stackoverflow.com/questions/53476305/attributeerror-tuple-object-has-no-attribute-log-softmax\n                outputs = model(inputs)\n            \n                loss = criterion(outputs, labels)\n            \n                loss.backward()\n                optimizer.step()\n        \n        loss_arr.append(loss.item())\n        print(\"\\ntraining completed for epoch:\",epoch+1,\"Validation started at:\",time.ctime())\n        \n        for param in model.parameters():\n            param.requires_grad()=False\n        model.eval()\n        val_loss=[]\n        \n        for i,(inputs,labels) in enumerate(tqdm(valid,0)):\n            inputs,labels=inputs.to(device),labels.to(device)\n            output=model(inputs)\n            \n            loss=criterion(output,labels)\n            val_loss.append(loss.item())\n            learn_rate.batch_step(np.mean(val_loss))\n            \n        print(f'Epoch {epoch+1} completed {time.ctime()} , train loss: {np.mean(loss_arr):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n        \n        \nfor i,(train_idx,valid_idx) in enumerate(splits.split(train_labels['id_code'],train_labels['diagnosis'])):\n    print(\"\\n starting fold \",i+1)\n    train_ds=train_labels.iloc[train_idx]\n    valid_ds=train_labels.iloc[valid_idx]\n    model=model.to(device)\n    train_model(model,train_ds,valid_ds)\n    \n    \ntorch.save(model.state_dict(),'model.bin')\n    \ntest_ds=eye(sample,data_dir+'/test_images/',transform=test_trans)\ntest=torch.utils.data.DataLoader(test_ds,shuffle=True)\n\n## Taken from https://www.kaggle.com/ateplyuk/aptos-pytorch-starter-rnet50\n\n# Prediction\npredict = []\n\nfor param in model.parameters():\n    param.requires_grad()=False\nmodel.eval()\nfor i, (data, _) in enumerate(test):\n    data = data.cuda()\n    output = model(data)  \n    output = output.cpu().detach().numpy()    \n    predict.append(output[0])\n    \nsample['diagnosis'] = np.argmax(predict,axis=1)\n\nsample.to_csv(\"submission.csv\",index=False)\n\n\n\n    \n    \n        \n        \n    \n    \n    \n    \n    \n    \n    \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}