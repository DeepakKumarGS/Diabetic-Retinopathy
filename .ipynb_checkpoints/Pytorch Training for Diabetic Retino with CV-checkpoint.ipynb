{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is my first major image classification competition and I have tried to put forth all the learnings from my deep learning course.\n",
    "\n",
    "* References:\n",
    "\n",
    "1.https://www.kaggle.com/super13579/pytorch-nn-cyclelr-k-fold-0-897-lightgbm-0-899\n",
    "\n",
    "2.https://www.kaggle.com/artgor/basic-eda-and-baseline-pytorch-model/data\n",
    "\n",
    "3.https://www.kaggle.com/xhlulu/aptos-2019-densenet-keras-starter\n",
    "\n",
    "4.https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import time\n",
    "import seaborn as sns\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from PIL import Image,ImageFile\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset,DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "import albumentations\n",
    "from albumentations import torch as AT\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Import the labels,\n",
    "train_labels=pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\n",
    "test=pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\n",
    "sample=pd.read_csv(\"../input/aptos2019-blindness-detection/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels['id_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - No DR\n",
    "\n",
    "1 - Mild\n",
    "\n",
    "2 - Moderate\n",
    "\n",
    "3 - Severe\n",
    "\n",
    "4 - Proliferative DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of images with No DR is 1805 followed by Moderate DR and Mild DR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test['id_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS=5\n",
    "BATCH_SIZE=64\n",
    "IMAGE_SIZE=224\n",
    "data_dir=\"../input/aptos2019-blindness-detection/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_splits=5\n",
    "splits=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x,valid_x,train_y,valid_y=train_test_split(train_labels,train_labels['diagnosis'],test_size=0.2,shuffle=train_labels['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x.shape,train_y.shape,valid_x.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired from - https://www.kaggle.com/xhlulu/aptos-2019-densenet-keras-starter\n",
    "def display_samples(df, columns=4, rows=3):\n",
    "    fig=plt.figure(figsize=(5*columns, 4*rows))\n",
    "\n",
    "    for i in range(columns*rows):\n",
    "        image_name = '{}.png'.format(df.iloc[i,0])  \n",
    "        image_id = df.iloc[i,1]\n",
    "        img_path=data_dir+'/train_images/'\n",
    "        img_path=join(img_path,image_name)\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        \n",
    "        \n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.title(image_id)\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "display_samples(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data:\n",
    "class eye(Dataset):\n",
    "    def __init__(self,labels,directory,subset=False,transform=None):\n",
    "        self.labels=labels\n",
    "        self.directory=directory\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name='{}.png'.format(self.labels.iloc[idx,0])  \n",
    "        full_image_path=join(self.directory,img_name)\n",
    "        #print(f'\\nImage path:{full_image_path}')\n",
    "        #img = cv2.imread(img_name)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #image = self.transform(img)\n",
    "        #image = image['image']\n",
    "        image=Image.open(full_image_path)\n",
    "        image_resize = image.resize((224, 224), resample=Image.BILINEAR)\n",
    "        image=self.transform(image)\n",
    "        #image=image['image']\n",
    "        \n",
    "        image_label=self.labels.iloc[idx,1:].as_matrix().astype('float')\n",
    "        image_label=np.argmax(image_label)\n",
    "        \n",
    "        \n",
    "            \n",
    "        return [image,image_label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class eyeTest(Dataset):\n",
    "#     def __init__(self,data,transform=None):\n",
    "#         self.data = data\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_name='{}.png'.format(self.data.iloc[idx,0])  \n",
    "#         full_image_path=join(data_dir+'/test_images/',img_name)\n",
    "#         image=Image.open(full_image_path)\n",
    "#         image = image.resize((299, 299), resample=Image.BILINEAR)\n",
    "#         image=self.transform(image)\n",
    "#         return {'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_trans=transforms.Compose([transforms.RandomCrop(224),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation(degrees=10),transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485,0.485,0.485],std=[0.485,0.485,0.485])])\n",
    "test_trans=transforms.Compose([transforms.RandomCrop(224),transforms.ToTensor(),\n",
    "                                #transforms.RandomHorizontalFlip(),\n",
    "                                #transforms.RandomRotation(),\n",
    "                                transforms.Normalize(mean=[0.485,0.485,0.485],std=[0.485,0.485,0.485])])\n",
    "# train_ds=eye(train_x,data_dir+'/train_images/',transform=train_trans)\n",
    "# valid_ds=eye(valid_x,data_dir+'/train_images/',transform=train_trans)\n",
    "# test_ds=eye(sample,data_dir+'/test_images/',transform=train_trans)\n",
    "\n",
    "# train_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "# valid_dl=DataLoader(valid_ds,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n",
    "# test_dl=DataLoader(test_ds,shuffle=True,num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr, val = train_test_split(train_labels.diagnosis, stratify=train_labels.diagnosis, test_size=0.1)\n",
    "# train_sampler = SubsetRandomSampler(list(tr.index))\n",
    "# valid_sampler = SubsetRandomSampler(list(val.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds=eye(train_labels,data_dir+'/train_images/',transform=train_trans)\n",
    "\n",
    "    \n",
    "# train=torch.utils.data.DataLoader(train_ds,batch_size=64,sampler=train_sampler,num_workers=4)\n",
    "# valid=torch.utils.data.DataLoader(train_ds,batch_size=64,sampler=valid_sampler,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.resnet50()\n",
    "model.load_state_dict(torch.load(\"../input/pretrained-pytorch-models/resnet50-19c8e357.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final linear layer in resnet has 1000 out features.In our case,the total number of output classes is 5 .Therefore we need to modify the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_in_features=model.fc.in_features\n",
    "model.fc=nn.Linear(final_in_features,10,bias=True)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from https://github.com/anandsaha/pytorch.cyclic.learning.rate/blob/master/cls.py\n",
    "# This code is from https://github.com/thomasjpfan/pytorch/blob/401ec389db2c9d2978917a6e4d1101b20340d7e7/torch/optim/lr_scheduler.py\n",
    "# This code is under review at PyTorch and is to be merged eventually to make CLR available to all.\n",
    "# Tested with pytorch 0.2.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CyclicLR(object):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `batch_step` should be called after a batch has been used for training.\n",
    "    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for eachparam groups.\n",
    "            Default: 0.001\n",
    "        max_lr (float or list): Upper boundaries in the cycle for\n",
    "            each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function. Default: 0.006\n",
    "        step_size (int): Number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch. Default: 2000\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        last_batch_iteration (int): The index of the last batch. Default: -1\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>         train_batch(...)\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(dataloader):\n",
    "    total, correct = 0, 0\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs,aux = model(inputs)\n",
    "        _, pred = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (pred == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,x_train,x_val):\n",
    "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.99)\n",
    "    learn_rate=CyclicLR(optimizer, base_lr=0.01, max_lr=0.05,step_size=40, mode='triangular2',gamma=0.99994)\n",
    "    \n",
    "    train_ds=eye(x_train,data_dir+'/train_images/',transform=train_trans)\n",
    "    valid_ds=eye(x_val,data_dir+'/train_images/',transform=test_trans)\n",
    "    \n",
    "    train=torch.utils.data.DataLoader(train_ds,batch_size=64,shuffle=True)\n",
    "    valid=torch.utils.data.DataLoader(valid_ds,batch_size=64,shuffle=True)\n",
    "    dataiter=iter(train)\n",
    "    img,label=next(dataiter)\n",
    "    print(\"\\n Shape of image\",img.shape)\n",
    "    #print(\"\\n Shape of label\",label[1].item())\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(time.ctime(),'Epoch:',epoch+1)\n",
    "        loss_arr=[]\n",
    "        loss_epoch_arr=[]\n",
    "        for i, (inputs,labels) in enumerate(train,0):\n",
    "        \n",
    "        #inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "            #https://stackoverflow.com/questions/53476305/attributeerror-tuple-object-has-no-attribute-log-softmax\n",
    "                outputs = model(inputs)\n",
    "            \n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        print(\"\\ntraining completed for epoch:\",epoch+1,\"Validation started at:\",time.ctime())\n",
    "        model.eval()\n",
    "        val_loss=[]\n",
    "        \n",
    "        for i,(inputs,labels) in enumerate(valid,0):\n",
    "            inputs,labels=inputs.to(device),labels.to(device)\n",
    "            output=model(inputs)\n",
    "            \n",
    "            loss=criterion(output,labels)\n",
    "            val_loss.append(loss.item())\n",
    "            learn_rate.batch_step(np.mean(val_loss))\n",
    "            \n",
    "        print(f'Epoch {epoch+1} completed {time.ctime()} , train loss: {np.mean(loss_arr):.4f}, valid loss: {np.mean(val_loss):.4f}.')\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(train_idx,valid_idx) in enumerate(splits.split(train_labels['id_code'],train_labels['diagnosis'])):\n",
    "    print(\"\\n starting fold \",i+1)\n",
    "    train_ds=train_labels.iloc[train_idx]\n",
    "    valid_ds=train_labels.iloc[valid_idx]\n",
    "    model=model.to(device)\n",
    "    train_model(model,train_ds,valid_ds)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# loss_arr = []\n",
    "# loss_epoch_arr = []\n",
    "# max_epochs = 3\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     model.train()\n",
    "#     for i, (inputs,labels) in enumerate(train_dl,0):\n",
    "        \n",
    "#         #inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.set_grad_enabled(True):\n",
    "#         #https://stackoverflow.com/questions/53476305/attributeerror-tuple-object-has-no-attribute-log-softmax\n",
    "#             outputs,aux = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "        \n",
    "#         loss_arr.append(loss.item())\n",
    "        \n",
    "#     loss_epoch_arr.append(loss.item())\n",
    "        \n",
    "#     print('Epoch: %d/%d, Train acc: %0.2f, Valid acc: %0.2f' % (epoch, max_epochs, evaluation(train_dl), evaluation(valid_dl)))\n",
    "    \n",
    "    \n",
    "# plt.plot(loss_epoch_arr)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds=eye(sample,data_dir+'/test_images/',transform=test_trans)\n",
    "test=torch.utils.data.DataLoader(test_ds,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://www.kaggle.com/ateplyuk/aptos-pytorch-starter-rnet50\n",
    "\n",
    "# Prediction\n",
    "predict = []\n",
    "model.eval()\n",
    "for i, (data, _) in enumerate(test):\n",
    "    data = data.cuda()\n",
    "    output = model(data)  \n",
    "    output = output.cpu().detach().numpy()    \n",
    "    predict.append(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['diagnosis'] = np.argmax(predict,axis=1)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
